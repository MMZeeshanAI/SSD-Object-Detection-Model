{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 8s/step - bbox_predictions_mse: 0.3673 - class_predictions_accuracy: 0.9255 - loss: 3.3263 - val_bbox_predictions_mse: 0.2950 - val_class_predictions_accuracy: 1.0000 - val_loss: 3.1409\n",
      "Epoch 2/5\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 6s/step - bbox_predictions_mse: 0.3019 - class_predictions_accuracy: 1.0000 - loss: 3.1478 - val_bbox_predictions_mse: 0.2949 - val_class_predictions_accuracy: 1.0000 - val_loss: 3.1409\n",
      "Epoch 3/5\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 6s/step - bbox_predictions_mse: 0.3012 - class_predictions_accuracy: 1.0000 - loss: 3.1471 - val_bbox_predictions_mse: 0.2949 - val_class_predictions_accuracy: 1.0000 - val_loss: 3.1409\n",
      "Epoch 4/5\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 5s/step - bbox_predictions_mse: 0.3017 - class_predictions_accuracy: 1.0000 - loss: 3.1477 - val_bbox_predictions_mse: 0.2949 - val_class_predictions_accuracy: 1.0000 - val_loss: 3.1409\n",
      "Epoch 5/5\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 4s/step - bbox_predictions_mse: 0.3017 - class_predictions_accuracy: 1.0000 - loss: 3.1476 - val_bbox_predictions_mse: 0.2949 - val_class_predictions_accuracy: 1.0000 - val_loss: 3.1409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26a15ccb220>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Flatten, Dense, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Constants\n",
    "MAX_OBJECTS_PER_IMAGE = 20\n",
    "NUM_CLASSES = 20  \n",
    "img_width, img_height = 300, 300  \n",
    "\n",
    "# Read and convert YOLOv8 annotations to SSD format\n",
    "def read_yolo_annotation(file_path, img_width, img_height):\n",
    "    annotations = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            center_x = float(parts[1]) * img_width\n",
    "            center_y = float(parts[2]) * img_height\n",
    "            width = float(parts[3]) * img_width\n",
    "            height = float(parts[4]) * img_height\n",
    "            xmin = int(center_x - width / 2)\n",
    "            ymin = int(center_y - height / 2)\n",
    "            xmax = int(center_x + width / 2)\n",
    "            ymax = int(center_y + height / 2)\n",
    "            annotations.append([class_id, xmin, ymin, xmax, ymax])\n",
    "    return annotations\n",
    "\n",
    "def create_xml_annotation(img_filename, img_width, img_height, annotations, output_dir):\n",
    "    annotation = ET.Element(\"annotation\")\n",
    "    ET.SubElement(annotation, \"filename\").text = img_filename\n",
    "\n",
    "    size = ET.SubElement(annotation, \"size\")\n",
    "    ET.SubElement(size, \"width\").text = str(img_width)\n",
    "    ET.SubElement(size, \"height\").text = str(img_height)\n",
    "    ET.SubElement(size, \"depth\").text = \"3\"\n",
    "\n",
    "    for ann in annotations:\n",
    "        class_id, xmin, ymin, xmax, ymax = ann\n",
    "        obj = ET.SubElement(annotation, \"object\")\n",
    "        ET.SubElement(obj, \"name\").text = str(class_id)\n",
    "        ET.SubElement(obj, \"pose\").text = \"Unspecified\"\n",
    "        ET.SubElement(obj, \"truncated\").text = \"0\"\n",
    "        ET.SubElement(obj, \"difficult\").text = \"0\"\n",
    "\n",
    "        bndbox = ET.SubElement(obj, \"bndbox\")\n",
    "        ET.SubElement(bndbox, \"xmin\").text = str(xmin)\n",
    "        ET.SubElement(bndbox, \"ymin\").text = str(ymin)\n",
    "        ET.SubElement(bndbox, \"xmax\").text = str(xmax)\n",
    "        ET.SubElement(bndbox, \"ymax\").text = str(ymax)\n",
    "\n",
    "    tree = ET.ElementTree(annotation)\n",
    "    output_path = os.path.join(output_dir, img_filename.replace('.jpg', '.xml'))\n",
    "    tree.write(output_path)\n",
    "\n",
    "def convert_dataset(yolo_dir, img_dir, output_dir, img_width, img_height):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for yolo_file in os.listdir(yolo_dir):\n",
    "        if yolo_file.endswith('.txt'):\n",
    "            img_filename = yolo_file.replace('.txt', '.jpg')\n",
    "            yolo_path = os.path.join(yolo_dir, yolo_file)\n",
    "            annotations = read_yolo_annotation(yolo_path, img_width, img_height)\n",
    "            create_xml_annotation(img_filename, img_width, img_height, annotations, output_dir)\n",
    "\n",
    "# Convert train and val sets\n",
    "convert_dataset('Annotated_clean/Annotated/labels/train', 'Annotated_clean/Annotated/images/train', 'Annotated_clean/Annotated/annotations/train', img_width, img_height)\n",
    "convert_dataset('Annotated_clean/Annotated/labels/val', 'Annotated_clean/Annotated/images/val', 'Annotated_clean/Annotated/annotations/val', img_width, img_height)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "def load_data(img_dir, ann_dir, img_width, img_height):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    for img_file in os.listdir(img_dir):\n",
    "        if img_file.endswith('.jpg'):\n",
    "            img_path = os.path.join(img_dir, img_file)\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.resize(image, (img_width, img_height))\n",
    "            images.append(image)\n",
    "            \n",
    "            ann_file = img_file.replace('.jpg', '.xml')\n",
    "            ann_path = os.path.join(ann_dir, ann_file)\n",
    "            tree = ET.parse(ann_path)\n",
    "            root = tree.getroot()\n",
    "            annots = []\n",
    "            for obj in root.findall('object'):\n",
    "                class_id = int(obj.find('name').text)\n",
    "                bndbox = obj.find('bndbox')\n",
    "                xmin = int(bndbox.find('xmin').text)\n",
    "                ymin = int(bndbox.find('ymin').text)\n",
    "                xmax = int(bndbox.find('xmax').text)\n",
    "                ymax = int(bndbox.find('ymax').text)\n",
    "                annots.append([class_id, xmin, ymin, xmax, ymax])\n",
    "            annotations.append(annots)\n",
    "    return np.array(images), annotations\n",
    "\n",
    "train_images, train_annotations = load_data('Annotated_clean/Annotated/images/train', 'Annotated_clean/Annotated/annotations/train', img_width, img_height)\n",
    "val_images, val_annotations = load_data('Annotated_clean/Annotated/images/val', 'Annotated_clean/Annotated/annotations/val', img_width, img_height)\n",
    "\n",
    "# Preprocess annotations\n",
    "def preprocess_annotations(annotations, img_width, img_height, max_objects, num_classes):\n",
    "    class_targets = np.zeros((len(annotations), max_objects), dtype=np.int32)\n",
    "    bbox_targets = np.zeros((len(annotations), max_objects, 4), dtype=np.float32)\n",
    "    \n",
    "    for i, ann in enumerate(annotations):\n",
    "        classes = np.zeros((max_objects,), dtype=np.int32)\n",
    "        bboxes = np.zeros((max_objects, 4), dtype=np.float32)\n",
    "        \n",
    "        for j, obj in enumerate(ann[:max_objects]):\n",
    "            class_id, xmin, ymin, xmax, ymax = obj\n",
    "            classes[j] = class_id\n",
    "            bboxes[j] = [xmin / img_width, ymin / img_height, xmax / img_width, ymax / img_height]\n",
    "        \n",
    "        class_targets[i] = classes\n",
    "        bbox_targets[i] = bboxes\n",
    "    \n",
    "    return class_targets, bbox_targets\n",
    "\n",
    "train_class_targets, train_bbox_targets = preprocess_annotations(train_annotations, img_width, img_height, MAX_OBJECTS_PER_IMAGE, NUM_CLASSES)\n",
    "val_class_targets, val_bbox_targets = preprocess_annotations(val_annotations, img_width, img_height, MAX_OBJECTS_PER_IMAGE, NUM_CLASSES)\n",
    "\n",
    "# Define SSD model\n",
    "def ssd_model(input_shape, num_classes, max_objects):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    \n",
    "    # Feature extractor (simplified)\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(input_tensor)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    \n",
    "    class_predictions = Dense(max_objects * num_classes, activation='softmax')(x)\n",
    "    class_predictions = Reshape((max_objects, num_classes), name='class_predictions')(class_predictions)\n",
    "    \n",
    "    bbox_predictions = Dense(max_objects * 4, activation='sigmoid')(x)\n",
    "    bbox_predictions = Reshape((max_objects, 4), name='bbox_predictions')(bbox_predictions)\n",
    "\n",
    "    model = Model(inputs=input_tensor, outputs=[class_predictions, bbox_predictions])\n",
    "    return model\n",
    "\n",
    "input_shape = (img_width, img_height, 3)  # Input shape\n",
    "\n",
    "model = ssd_model(input_shape, NUM_CLASSES, MAX_OBJECTS_PER_IMAGE)\n",
    "model.compile(optimizer='adam', \n",
    "              loss={'class_predictions': 'sparse_categorical_crossentropy', 'bbox_predictions': 'mse'},\n",
    "              metrics={'class_predictions': 'accuracy', 'bbox_predictions': 'mse'})\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, {'class_predictions': train_class_targets, 'bbox_predictions': train_bbox_targets},\n",
    "          validation_data=(val_images, {'class_predictions': val_class_targets, 'bbox_predictions': val_bbox_targets}),\n",
    "          epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 494ms/step - bbox_predictions_mse: 0.2913 - class_predictions_accuracy: 1.0000 - loss: 3.1372\n",
      "Validation Loss: 3.1408536434173584\n",
      "Validation Class Accuracy: 1.0\n",
      "Validation BBox MSE: 0.2949081361293793\n"
     ]
    }
   ],
   "source": [
    "# Validate and output performance data\n",
    "performance = model.evaluate(val_images, {'class_predictions': val_class_targets, 'bbox_predictions': val_bbox_targets})\n",
    "print(f\"Validation Loss: {performance[0]}\")\n",
    "print(f\"Validation Class Accuracy: {performance[2]}\")\n",
    "print(f\"Validation BBox MSE: {performance[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.  Setting Up and Data Handling:**\n",
    "\n",
    "* It defines constants like the maximum number of objects per image (`MAX_OBJECTS_PER_IMAGE`), number of classes (`NUM_CLASSES`), and image width and height (`img_width`, `img_height`).\n",
    "* It has functions to:\n",
    "    * Read YOLOv8 annotations (a different object detection format) and convert them to SSD format (`read_yolo_annotation`).\n",
    "    * Create an XML annotation file for an image containing bounding boxes for detected objects (`create_xml_annotation`).\n",
    "    * Convert a whole dataset (images and annotations) from YOLOv8 format to SSD format (`convert_dataset`).\n",
    "* It also defines functions to: \n",
    "    * Load and preprocess images (`load_data`). This resizes images and parses the corresponding XML annotations. \n",
    "    * Preprocess annotations to a fixed format suitable for training the model (`preprocess_annotations`). \n",
    "\n",
    "**2. Building the SSD Model:**\n",
    "\n",
    "* It defines a function `ssd_model` that builds the SSD model architecture.\n",
    "  * This is a simplified example with only a few convolutional layers (`Conv2D`), pooling layers (`MaxPooling2D`), a fully connected layer (`Dense`), and reshaping layers (`Reshape`).\n",
    "  * The model has two outputs:\n",
    "      * Class predictions: This predicts the class probabilities for each object (one-hot encoded with `softmax`).\n",
    "      * Bounding box predictions: This predicts the offsets for bounding boxes relative to the image size (using sigmoid activation).\n",
    "\n",
    "**3. Training and Evaluation:**\n",
    "\n",
    "* It defines the model input shape based on the image dimensions.\n",
    "* It creates an SSD model instance with the defined architecture.\n",
    "* It compiles the model with the Adam optimizer, a combination of loss functions for classification (categorical crossentropy) and bounding box prediction (mean squared error), and accuracy and mean squared error metrics for both outputs.\n",
    "* It trains the model on the preprocessed training images and annotations (`train_images`, `train_class_targets`, `train_bbox_targets`).\n",
    "* It evaluates the model on the validation set (`val_images`, `val_class_targets`, `val_bbox_targets`) and prints the validation loss, class accuracy, and bounding box mean squared error.\n",
    "\n",
    "**Overall, this code snippet demonstrates how to convert annotations from a different format (YOLOv8) to SSD format, preprocess data for training, build a basic SSD model architecture, train it on a dataset, and evaluate its performance.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
